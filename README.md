## POC Perguntas e Respostas em portugu√™s utilizando BERT

Um dos maiores desafios no campo de processamento de linguagem natural (NLP) √© a automatiza√ß√£o de processos que buscam o entendimento da sem√¢ntica e compreens√£o de textos. Modelos de rede neurais aplicados a transfer√™ncia de aprendizado tem conseguido √≥timos resultados na tarefa de identifica√ß√£o de contexto. A arquitetura dos Transformers, um modelo de aprendizado profundo, no qual cada elemento de sa√≠da √© conectado a cada elemento de entrada, al√©m do mecanismo de ‚Äúaten√ß√£o‚Äù (Attention) que identifica as pondera√ß√µes entre eles, foram os respons√°veis pela grande evolu√ß√£o e s√£o a base dos diversos modelos de classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes, resposta a perguntas e gera√ß√£o de texto.

Os Transformers foram introduzidos pelo Google em 2017 (paper: Attention Is All You Need). Na √©poca, os modelos de linguagem usavam principalmente redes neurais recorrentes (RNN) e redes neurais convolucionais (CNN) para lidar com tarefas de NLP. Embora esses modelos tenham bons resultados, o Transformer/Attention √© considerado uma evolu√ß√£o importante.

Enquanto os modelos convencionais de redes neurais necessitam que as sequ√™ncias de dados sejam processados em ordem fixa, o modelo Transformer processa os dados em qualquer ordem, permitindo uma efici√™ncia maior no processo de treinamento, al√©m de utilizar estrat√©gias como a MLM (Masked Language Model) que faz com o que o modelo identifique uma palavra ‚Äúmascarada‚Äù baseando-se no contexto em que ela est√° inserida. Isso permite que o modelo vincule as palavras ao contexto das senten√ßas em que est√£o inseridas e com isso responder melhor as pesquisas que s√£o feitas. O mecanismo de aten√ß√£o (Attention) desempenha um papel importante em enfatizar em qual parte do contexto o modelo deve se concentrar.

Essa flexibilidade permitiu que os modelos Transformes fossem pr√©-treinados, criando uma camada de "conhecimento", que a partir do processo de transfer√™ncia de aprendizado, pode ser adaptado aos conte√∫dos pesquisados (fine tunning) obtendo-se respostas melhores do modelo.

O primeiro modelo lan√ßado (2018) foi o BERT (Bidirectional Encoder Representations from Transformers), com o seu c√≥digo aberto e uma quantidade consider√°vel de dados pr√©-treinados. Atualmente existem dezenas de modelos diferentes (RoBERTa, DistilBERT, GTP, GTP-2, etc...) que foram aprimorados pelas grandes empresas do mercado (Google, Facebook, Microsoft) e por startups de tecnologia como Huggingface e OpenAI.

Implementamos nesta prova de conceito de Perguntas e Respostas o modelo BERT, utilizando a biblioteca Transformers da Huggingface (ü§ó), sobre um texto extra√≠do da wikipidea. Em fun√ß√£o do custo computacional para realiza√ß√£o do treinamento e o fine tunning de um novo modelo, buscamos modelos j√° pr√©-treinados na base da Huggingface. Os modelos padr√µes (sem fine tunning) de BERT (large e Base) n√£o tiveram resposta satisfat√≥ria. Temos poucas op√ß√µes de modelos pr√©-treinados em portugu√™s, sendo a maioria tradu√ß√µes automatizadas de modelos em ingl√™s, o que limita a utiliza√ß√£o de modelos pr√©-treinados para o Portugu√™s.

O modelo (mrm8488/bert-base-portuguese-cased-finetuned-squad-v1-pt) disponibilizado pelo usu√°rio Manuel Romero (mrm8488 do github) foi o que trouxe os melhores resultados.

A Hugging Face √© uma startup focada em NLP com uma grande comunidade de c√≥digo aberto. Eles desenvolveram uma biblioteca (transformers) baseada em python que disponibiliza uma API para as principais arquiteturas conhecidas, como BERT, RoBERTa, GPT-2 ou DistilBERT, que est√£o sendo utilizadas, com resultados de √∫ltima gera√ß√£o em uma variedade de tarefas de NLP como: classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes, resposta a perguntas e gera√ß√£o de texto. Essas arquiteturas j√° possuem diversos corpus pr√©-treinados em diversas l√≠nguas.

