# Modelo Bert de Perguntas e Respostas

#### Aluno: Nelson Cust√≥dio da Silveira Neto (https://github.com/ncsneto)
#### Orientador(/a/es/as): PhD Leonardo Alfredo Forero Mendoza (prof.leonardo@ica.ele.puc-rio.br) e Cristian Munoz (prof.cristian@ica.ele.puc-rio.br).

---

Trabalho apresentado ao curso [BI MASTER](https://ica.puc-rio.ai/bi-master) como pr√©-requisito para conclus√£o de curso e obten√ß√£o de cr√©dito na disciplina "Projetos de Sistemas Inteligentes de Apoio √† Decis√£o".

- [Link para o c√≥digo](https://github.com/ncsneto/Question-Answer_BERT). 

- Trabalhos relacionados:
    - [Google Research Project](https://github.com/google-research/bert#pre-trained-models).
    - [Building a QA System with BERT on Wikipedia](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html#QA-on-Wikipedia-pages).
    - [Stanford Question Answering Dataset] (https://rajpurkar.github.io/SQuAD-explorer/)
    - [PyTorch Question Answering] (https://github.com/kushalj001/pytorch-question-answering#pytorch-question-answering)
    - [LSTM is dead. Long Live Transformers!] (https://www.youtube.com/watch?v=S27pHKBEp30)
    - [Attention Is All You Need] (https://arxiv.org/abs/1706.03762) (https://www.youtube.com/watch?v=iDulhoQ2pro)
    - [Transformer Neural Networks - EXPLAINED! (Attention is all you need)] (https://www.youtube.com/watch?v=TQQlZhbC5ps&t=13s)
    - [pytorch-question-answering] (https://github.com/kushalj001/pytorch-question-answering/blob/master/1.%20DrQA.ipynb)
    - [BERTimbau - Portuguese BERT] (https://github.com/neuralmind-ai/portuguese-bert)
    - [Portuguese Named Entity Recognition using BERT-CRF] (https://github.com/neuralmind-ai/portuguese-bert)
    - [NLP Tutorial: Creating Question Answering System using BERT + SQuAD on Colab TPU] (https://hackernoon.com/nlp-tutorial-creating-question-answering-system-using-bert-squad-on-colab-tpu-1utp3352)
    - [Portuguese Word Embeddings] (http://www.davidsbatista.net/blog/2019/11/03/Portuguese-Embeddings/)
    - [Pergunta e resposta de BERT] (https://www.tensorflow.org/lite/examples/bert_qa/overview)
    - [Answering Questions With Transformers] (https://predictivehacks.com/answering-questions-with-transformers/)
    - [RAPPORT: A Fact-Based Question Answering System for Portuguese] (https://estudogeral.uc.pt/handle/10316/41880)
    - [Question Answering] (https://paperswithcode.com/task/question-answering)
    - [Fine-tuning a model on a question-answering task] (https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)
    - [Datasets em Portugu√™s] (https://forum.ailab.unb.br/t/datasets-em-portugues/251)
    - [Building a QA System with BERT on Wikipedia](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html#QA-on-Wikipedia-pages).

---

### Resumo

POC Perguntas e Respostas em portugu√™s utilizando BERT

Um dos maiores desafios no campo de processamento de linguagem natural (NLP) est√° na automatiza√ß√£o de processos que buscam o entendimento da sem√¢ntica e compreens√£o de textos. Modelos de rede neurais aplicados a transfer√™ncia de aprendizado tem conseguido √≥timos resultados na tarefa de identifica√ß√£o de contexto. A arquitetura dos Transformers, um modelo de aprendizado profundo, no qual cada elemento de sa√≠da √© conectado a cada elemento de entrada, al√©m do mecanismo de ‚Äúaten√ß√£o‚Äù (Attention) que identifica as pondera√ß√µes entre eles, foram os respons√°veis pela grande evolu√ß√£o e s√£o a base dos diversos modelos de classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes, resposta a perguntas e gera√ß√£o de texto.

Os Transformers foram introduzidos pelo Google em 2017 (paper: Attention Is All You Need). Na √©poca, os modelos de linguagem usavam principalmente redes neurais recorrentes (RNN) e redes neurais convolucionais (CNN) para lidar com tarefas de NLP. Embora esses modelos tenham bons resultados, o Transformer/Attention √© considerado uma evolu√ß√£o importante.

Enquanto os modelos convencionais de redes neurais necessitam que as sequ√™ncias de dados sejam processados em ordem fixa, o modelo Transformer processa os dados em qualquer ordem, permitindo uma efici√™ncia maior no processo de treinamento, al√©m de utilizar estrat√©gias como a MLM (Masked Language Model) que faz com o que o modelo identifique uma palavra ‚Äúmascarada‚Äù baseando-se no contexto em que ela est√° inserida. Isso permite que o modelo vincule as palavras ao contexto das senten√ßas em que est√£o inseridas e com isso responder melhor as pesquisas que s√£o feitas. O mecanismo de aten√ß√£o (Attention) desempenha um papel importante em enfatizar em qual parte do contexto o modelo deve se concentrar.

Essa flexibilidade permitiu que os modelos Transformes fossem pr√©-treinados, criando uma camada de "conhecimento", que a partir do processo de transfer√™ncia de aprendizado, pode ser adaptado aos conte√∫dos pesquisados (fine tunning) obtendo-se respostas melhores do modelo.

O primeiro modelo lan√ßado (2018) foi o BERT (Bidirectional Encoder Representations from Transformers), com o seu c√≥digo aberto e uma quantidade consider√°vel de dados pr√©-treinados. Atualmente existem dezenas de modelos diferentes (RoBERTa, DistilBERT, GTP, GTP-2, etc...) que foram aprimorados pelas grandes empresas do mercado (Google, Facebook, Microsoft) e por startups de tecnologia como Huggingface e OpenAI.

Implementamos nesta prova de conceito de Perguntas e Respostas o modelo BERT, utilizando a biblioteca Transformers da Huggingface (ü§ó), sobre um texto extra√≠do da wikipidea. Em fun√ß√£o do custo computacional para realiza√ß√£o do treinamento e o fine tunning de um novo modelo, buscamos modelos j√° pr√©-treinados na base da Huggingface (ü§ó). Os modelos padr√µes (sem fine tunning) de BERT (large e Base) n√£o tiveram resposta satisfat√≥ria. Temos poucas op√ß√µes de modelos pr√©-treinados em portugu√™s, sendo a maioria tradu√ß√µes automatizadas de modelos em ingl√™s, o que limita a utiliza√ß√£o de modelos pr√©-treinados para o Portugu√™s.

O modelo (mrm8488/bert-base-portuguese-cased-finetuned-squad-v1-pt) disponibilizado pelo usu√°rio Manuel Romero (mrm8488 do github) foi o que trouxe os melhores resultados.

A Hugging Face (ü§ó) √© uma startup focada em NLP com uma grande comunidade de c√≥digo aberto. Eles desenvolveram uma biblioteca (transformers) baseada em python que disponibiliza uma API para as principais arquiteturas conhecidas, como BERT, RoBERTa, GPT-2 ou DistilBERT, que est√£o sendo utilizadas, com resultados de √∫ltima gera√ß√£o em uma variedade de tarefas de NLP como: classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes, resposta a perguntas e gera√ß√£o de texto. Essas arquiteturas j√° possuem diversos corpus pr√©-treinados em diversas l√≠nguas.


---

Matr√≠cula: 191.671.022

Pontif√≠cia Universidade Cat√≥lica do Rio de Janeiro

Curso de P√≥s Gradua√ß√£o *Business Intelligence Master*
